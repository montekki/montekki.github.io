+++
title = "Efficient IP for MatMult"

[extra]
images = [
    "../static/images/figure_4_6.png"
]

[taxonomies]
tags = ["rust", "zk", "pazk"]
+++

Hi how about some performance? In Chapter 4 the book first introduces
a regular MatMult IP that has already been implemented in
[previous post](@/2022-08-22-thaler-ch4-2.md) and then discusses
the performance improvements to it. Should be fun, let's dive right in.

<!-- more -->

The Book first discusses a general method for MatMult IP that
gives us a Prover's total runtime of $\mathcal{O}(n^3)$ and then
goes on about ideas on shaving that off to $\mathcal{O}(n^2)$.

This less formal discussion illustrates the ideas on the example
of a matrix multiplication but then generalizes them in three lemmas.
Let's discuss them and what form they take in code.


### Lemma 4.3

Suppose that $p$ is an $l$-variate multilinear polynomial over
field $\mathbb{F}$ and that $A$ is an array of length $2^l$ such that
$\forall x \in \lbrace 0; 1 \rbrace ^l, A[x] = p(x)$.

Then for any $r_1 \in \mathbb{F}$ there is an algorithm running in
time $\mathcal{O}(2^l)$, that given $r_1$ and $A$ as input, computes an
array $B$ of length $2^{l-1}$ such that
$\forall x' \in \lbrace 0; 1 \rbrace ^{l-1}, B[x'] = p(r_1, x')$.

Proof reminds us that the multilinear polynomial $p(x_1,x_2,\cdots,x_n)$
can be expressed via:

$$
p(x_1,x_2,\cdots,x_n) = x_1 \cdot p(1,x_1,\cdots,x_n) +
(1 - x_1) \cdot p(0,x_2,\cdots,x_n)
$$

The algorithm to compute $B$ iterates over every value
$x' \in \lbrace 0; 1 \rbrace ^{l-1}$ and sets

$$
B[x'] \leftarrow r_1 \cdot A[1;x'] + (1 - r_1)\cdot A[0;x']
$$

Lets try to unpack this and understand how would that like in code.

If we have multilinear extensions in an evaluations form where
by indexing into an array of evaluations at index $i$ we get
the evaluation of the polynomial at point
$x = \lbrace 0; 1 \rbrace ^ {\text{num vars}}$ where $i$ encodes
$x$ in binary form. Then the above algorithm would look something
like this (more of a pseudocode, i haven't tested this exact code):

```rust
fn fix_variable<F: Field>(a: &[A], r_1: F) -> Vec<F> {
    // A has length 2^l and as such B should have lenth 2^{l-1}.
    // We can calculate it by a bitwise shift one position
    // to the right
    let b_length = (a.len() >> 1);

    // Then we pre-allocate B with a necessary capacity
    let mut b = Vec::with_capacity(b_length);

    // Iterate over all indices of B and perform computation as above
    for x in 0..b_length {
        let b_i = r_1 * a[(x << 1) + 1] + (F::one() - r_1) * a[x << 1];
        b.push(b_i)
    }

    b
}
```

Looks not that bad, the only non-obvious thing here is the indexing
into the $A$ array. Remember that to compute $B[x']$ we need to index
into a at two points: $A[0;x']$ and $A[1;x']$. Within the bit-representation
of $(x_1,\cdots,x_n)$ the first variable $x_1$ corresponds to the
Least Significant Bit and so we can construct the indices into $A$
by shifting the index $x'$ one bit right and setting the LSB to $1$ or $0$.

One could imagine a more general implementation of this algorithm
where instead of fixing just one first variable $r_1$ in the polynomial
any number $i < n$ of first variables could be fixed in place:
$A[r_1,\cdots,r_i,x_{i+1},\cdots,x_{n}]$. That can be done by "fixing"
one variable at a time:

Compute

$$
A[r_1,x_2,\cdots,x_n]
$$

Use the result to compute

$$
A[r_2,x_3,\cdots,x_n]
$$

and so on.

This is exactly what the method [`fix_variables()`](
https://github.com/arkworks-rs/algebra/blob/4d485734751a887caffda8c8c75147ab796d8922/poly/src/evaluations/multivariate/multilinear/dense.rs#L113-L129
)
of [`DenseMultilinearExtension`] is doing. That is neat 
and looks like no need to implement this part on our own!.

[`DenseMultilinearExtension`]: https://docs.rs/ark-poly/0.3.0/ark_poly/struct.DenseMultilinearExtension.html

### Lemmas 4.4 and 4.5

Lemmas 4.4 and 4.5 give us the runtime of the Prover for the polynomial of
form $g = p_1, p_2, \cdots, p_k$ where $p_i$ are multilinear polynomials.
It is certainly useful to dive into the proofs of these lemmas but the
main takeaway from them for us is best depicted in this Figure 4.6
that gives us the recipe for what we have to do.

![Figure 4.6](../figure_4_6.png)

With this figure it easy to see the main idea: in each round of
the algorithm it "fixes" the next variable in the multilinear polynomial
and uses the result of the previous iteration to do that. As such
at each iteration we will get all the necessary evaluations of the
polynomial $g(r_1,\cdots,r_i,x_{i+1},\cdots,x_n)$ over a boolean
hypercube at Prover's step $i$ of the SumCheck protocol.

### Modifying the `SumCheckPolynomial` trait

Recall that in the [previous post](@/2022-08-22-thaler-ch4-2.md) 
a generalized trait for the polynomial was introduced. It does not
really fit the algorithm described above and even speaking more
generally: at each step $i$ of the SumCheck protocol it re-evaluates
polynomial $g$ over each fixed variable so far $r_1,\cdots,r_i$
and the boolean hypercube of the values of the variables not fixed so far.

But regardless of the polynomial we are dealing with "fixing" variable, say,
$r_1$ more than once is quite excessive, but it would be "fixed" $n$ times.
This fact could be improved by introducing a method similar to `fix_variables`
from the [`DenseMultilinearExtension`].

With that going to a univariate polynomial would make sense for
only the first variable in the polynomial.

The overall changed trait would look something like this:

```rust
pub trait SumCheckPolynomial<F: Field> {
    fn evaluate(&self, point: &[F]) -> Option<F>;

    fn fix_variables(&self, partial_point: &[F]) -> Self;

    fn to_univariate(&self) -> univariate::SparsePolynomial<F>;

    fn num_vars(&self) -> usize;

    fn to_evaluations(&self) -> Vec<F>;
}
```

This change was added in 
[`206039c`](
https://github.com/montekki/thaler-study/commit/206039ca74b0307b9fe90c382530daadd85db6ef
)

### An efficient MatMult algorithm

The change above would actually greatly simplify the previous
code for MatMult _and_ it would allow us to write an efficient
version of it.

The only remaining non-trivial function would be `to_univariate`:

```rust
fn to_univariate(&self) -> SparsePolynomial<F> {
    let domain: GeneralEvaluationDomain<F> = GeneralEvaluationDomain::new(3).unwrap();

    let evals = domain
        .elements()
        .map(|e| {
            let f_a_evals = self.f_a.fix_variables(&[e]).to_evaluations();
            let f_b_evals = self.f_b.fix_variables(&[e]).to_evaluations();
            f_a_evals
                .into_iter()
                .zip(f_b_evals.into_iter())
                .map(|(a, b)| a * b)
                .sum()
        })
        .collect();

    let evaluations = Evaluations::from_vec_and_domain(evals, domain);
    let p = evaluations.interpolate();

    p.into()
}
```

These changes have been implemented in [`569a73a`](
https://github.com/montekki/thaler-study/commit/569a73ac4c849928f3a4a15b9fce98323fd7d3d4
)

### Benchmarking the implementation

With a simple benchmark with `criterion` we may see a following runtime
of the Prover:

![Prover Runtime](../matmul_benchmarks.svg)

Looks ok!
